---
title: "Capstone Project"
author: "Yash Bhanushali"
date: "28/07/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Import any necessary packages

1)Importing all necessary packages.

2)Read in the data as an .csv file

3)Display the head of the data (first 6 rows) so we can begin to understand what the dataset looks like.

```{r}
library(readr)     #For reading csv files
library(dplyr)     #For data wrangling
library(lubridate) #For handling dates
library(ggplot2)   #For plotting of results
```


```{r}
setwd("C:/Users/shari/OneDrive/Desktop/Business Analytics/Capstone")

Online_Retail <- read.csv("C:/Users/shari/OneDrive/Desktop/Business Analytics/Capstone/Online_Retail.csv")
```


```{r}
#Convert to dataframe
Online_Retail = data.frame(Online_Retail)
#Preview the data
head(Online_Retail)
```


##Data Manipulation in dplyr
1) The transactions that are not made with a logged-in user have been deleted because it is difficult to find out how many repeat purchases a single user made if it checked out as a guest 

2)To make sure that all the columns are of the appropriate data type; the invoice date column must be coverted to a date type.

3) The data which is in transactional level (the online retail dataframe) is being transformed into customer-level (customelevl dataframe)


```{r}
# Finding out the number of rows that are not linked with logged-in users.
sum(is.na(Online_Retail$CustomerID))
#Answer: 135,080 transactions are not linked with logged-in users. 
```

# Our aim here is to monitor the customer behaviour these transactions which are of guest (not logged in) users are of no use thus we are excluding it.

```{r}
Online_Retail <- Online_Retail %>%
  filter(!is.na(CustomerID))
```

```{r}
#Ensure columns are of correct types
str(Online_Retail)
#In my offline version of the data (downloaded csv), I had to convert the invoice date column to a date column. Here, it looks like the correct
#datatypes have been preserved.

#Following is the count of unique items purchased by each customer.
uniqueitems <- Online_Retail %>%
  group_by(CustomerID) %>%
  summarise(unique_items_customer = n_distinct(StockCode))

#Grouping the data at an invoice level
invoicelvl <- Online_Retail %>%
  group_by(CustomerID, InvoiceNo) %>%
  summarise(unique_items_inv = n(),
            quantity_purchased = sum(Quantity),
            total_price = sum(Quantity*UnitPrice),
            invoice_date = first(InvoiceDate)) %>%
  arrange(CustomerID,invoice_date)

#Joining the two arrays together
combineddata <- left_join(invoicelvl, uniqueitems, by='CustomerID')

#Grouping the data at a customer level
customerlvl <- combineddata %>%
    group_by(CustomerID) %>%
    summarise(no_orders = n(),
              unique_items_purchased = min(unique_items_customer),
              quantity_items_purchased = sum(quantity_purchased),
              average_quantity_per_order = mean(quantity_purchased),
              total_money_spent = sum(total_price),
              average_spent_per_order = mean(total_price))
```


#The data required to group 4,327 users into different clusters is almost ready. Following is the overview of the new dataset

```{r}
summary(customerlvl[2:7])
```


It is showing the value of total money spent is substantially larger than the rest of the columns now if k-means was used to the unscaled data, total money spent would have dominated the rest of the columns, and our customers would have been categorized majorly by the total money spent. Thus scaling the data is required. 


```{r}
#Scale the Data
km_data <- scale(customerlvl[2:7])
summary(km_data)
```
##Dimensionality is being reduced now

In this step the following things would be done:
1)Check for multicollinearity.

2)Try to find principal components to see if the dimensionality can be reduced.


**Checking for multicollinearity**
```{r}
#Print correlation matrix
cor(km_data)
```

Total_money_spent has been eliminated because it is significantly correlated with quantity_items_purchased and we do not require both of these variables for cluster analysis.

```{r}
#Remove the 5th column & scale
km_data_reduced <- scale(customerlvl[c(2:5,7)])
summary(km_data_reduced)
```

**Checking for Principal Components**
```{r}
km_data.pca <- prcomp(km_data_reduced, center = T,scale = T)
#Summary of Principal Components
summary(km_data.pca)
#Proportion of variance explained:
var <- km_data.pca$sdev^2
pve <- var/sum(var)
#Scree Plot of Cumulative Variance Explained:
g <- qplot(x = 1:5, y = cumsum(pve), geom = 'line', xlab="Number of Principal Components", ylab='Proportion of Variance Explained', main="Scree Plot of Proportion of Variance Explained")
g + scale_x_continuous(breaks = seq(0, 5, by = 1))
```
##Applying K-means

**Identify the optimal number of clusters**
```{r}
#Remove randomness from iterations by setting a seed
set.seed(123)

#Determining the maximum number of clusters
#k_max = 10 means we will test all possible numbers of clusters from 1 to 10 to see which performs the best.
k_max <- 10

#Calculate the total within sum of squares for each of 1:k_max
twcss <- sapply(1:k_max, function(k){kmeans(km_data_reduced, k)$tot.withinss})
#Vizualize the results
g <- qplot(x = 1:k_max, y = twcss, geom = 'line', xlab="Number of Clusters", ylab='TWCSS', main="Scree Plot of Number of Clusters")
g + scale_x_continuous(breaks = seq(0, 10, by = 1))
```
We're looking for a 'elbow' in the plot where the model's quality no longer improves considerably as the model's complexity grows. There is some ambiguity in this plan because there isn't a defined elbow. This is due to a lack of separation between clusters (see the pairs plot below).


I'll use k=5 in this case.After testing the model we shall find out if this assumption was a good or a bad call.

**Applying K-means clustering with 5 cluster**
```{r}
# k-means clustering
km <-kmeans(km_data_reduced, centers = 5, nstart=20)
#Check total within sum of squares with a variety of nstart and iter.max values
#to ensure algorithm convergence
km$tot.withinss
#What are the characteristics of the 5 groups?
km$centers
#How many users belong to each group?
table(km$cluster)
# plot the dataset with clusters
par(pty="m")
pairs(km_data, col = km$cluster, main="K-Means with 5 Clusters", lower.panel=NULL,
      xaxt="n",yaxt="n", oma=c(0,0,5,0))
```
This scale is really hard to interpret - for example, category 2, where the 'number of orders' variable is centred around a negative number. Transforming centers back to original scale for added interpretability.



**Backtransforming on to original scale of variables**
```{r}
data.orig = t(apply(km$centers, 1, function(r)r*attr(km_data_reduced,'scaled:scale') + attr(km_data_reduced, 'scaled:center')))
print(data.orig)
```

